{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'''\n",
    "python mariadb 查询数据 Demo\n",
    "'''\n",
    "\n",
    "import pymysql  \n",
    "  \n",
    "# 创建连接  \n",
    "connection = pymysql.connect(  \n",
    "    host='localhost',  # 数据库服务器地址  \n",
    "    user='root',   # 数据库用户名  \n",
    "    password='root', # 数据库密码  \n",
    "    database='bilibili',  # 数据库名  \n",
    "    charset='utf8mb4',  # 字符集  \n",
    "    cursorclass=pymysql.cursors.DictCursor  # 返回字典形式的游标  \n",
    ")  \n",
    "  \n",
    "try:  \n",
    "    with connection.cursor() as cursor:  \n",
    "        # 在此处执行 SQL 查询  \n",
    "        sql = \"SELECT `id`, `username` FROM `test` \"  \n",
    "        cursor.execute(sql)  \n",
    "        result = cursor.fetchone()  \n",
    "        print(result)  \n",
    "finally:  \n",
    "    connection.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "查询哔哩哔哩UP主首页的视频信息，并存入到数据库中\n",
    "video_name：视频名称\n",
    "play_count：播放数量\n",
    "updated：更新日期\n",
    "picture：封面地址\n",
    "description：视频描述\n",
    "comment：评论数量\n",
    "bvid:BV 号\n",
    "\n",
    "使用 pandas 来进行数据的存入更简单一些\n",
    "\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import pymysql\n",
    "\n",
    "# 创建连接  \n",
    "connection = pymysql.connect(  \n",
    "    host='localhost',  # 数据库服务器地址  \n",
    "    user='root',   # 数据库用户名  \n",
    "    password='root', # 数据库密码  \n",
    "    database='bilibili',  # 数据库名  \n",
    "    charset='utf8mb4',  # 字符集  \n",
    "    cursorclass=pymysql.cursors.DictCursor  # 返回字典形式的游标  \n",
    ")  \n",
    "\n",
    "# 数据接口的URL  \n",
    "url = 'https://api.bilibili.com/x/space/wbi/arc/search?mid=51420401&pn=1&ps=25&index=1&order=pubdate&order_avoided=true&platform=web&web_location=1550101&dm_img_list=[%7B%22x%22:550,%22y%22:-87,%22z%22:0,%22timestamp%22:4,%22k%22:107,%22type%22:0%7D,%7B%22x%22:951,%22y%22:-634,%22z%22:48,%22timestamp%22:161,%22k%22:88,%22type%22:0%7D]&dm_img_str=V2ViR0wgMS4wIChPcGVuR0wgRVMgMi4wIENocm9taXVtKQ&dm_cover_img_str=QU5HTEUgKEFwcGxlLCBBcHBsZSBNMSwgT3BlbkdMIDQuMSlHb29nbGUgSW5jLiAoQXBwbG&dm_img_inter=%7B%22ds%22:[%7B%22t%22:0,%22c%22:%22%22,%22p%22:[306,102,102],%22s%22:[81,4951,4822]%7D],%22wh%22:[3508,5511,26],%22of%22:[197,394,197]%7D&w_rid=65cd76a1cb55f0c43636f374285832b0&wts=1713265842'  \n",
    "  \n",
    "# 设置请求头，模拟浏览器行为  \n",
    "headers = {  \n",
    "    # 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0'\n",
    "    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0'\n",
    "}  \n",
    "  \n",
    "# 如果需要处理Cookies，可以从浏览器获取后设置  \n",
    "# cookies = {'cookie_name': 'cookie_value'}  \n",
    "  \n",
    "# 发送GET请求，带上请求头  \n",
    "response = requests.get(url, headers=headers)  # 如果需要带上Cookies，可以加上cookies=cookies参数  \n",
    "\n",
    "# 检查请求是否成功  \n",
    "if response.status_code == 200:  \n",
    "    # 解析JSON响应（如果接口返回的是JSON）  \n",
    "    json_data = response.json()\n",
    "    # 获取节目列表\n",
    "    data=json_data[\"data\"][\"list\"]['vlist']\n",
    "    print(type(data))\n",
    "    \n",
    "    try:\n",
    "        with connection.cursor() as cursor: \n",
    "            # 在此处执行 SQL 查询 \n",
    "            for item in data:\n",
    "                sql = \"INSERT INTO updata (video_name, play_count, updated, picture, description, comment, bvid) VALUES (%s, %s, %s, %s, %s, %s, %s)\" \n",
    "                print(item[\"title\"])\n",
    "                print(type(item[\"title\"]))\n",
    "                values=(item[\"title\"], \n",
    "                        item[\"play\"], \n",
    "                        item[\"created\"], \n",
    "                        item[\"pic\"], \n",
    "                        item[\"description\"], \n",
    "                        item[\"comment\"], \n",
    "                        item[\"bvid\"])\n",
    "                cursor.execute(sql, values) \n",
    "            connection.commit()  \n",
    "    finally:\n",
    "        connection.close()\n",
    "        print(\"Data inserted successfully.\") \n",
    "else:  \n",
    "    print(f\"请求失败，状态码：{response.status_code}\")"
   ],
   "id": "5362b5f33a9f59b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T09:17:52.916892Z",
     "start_time": "2024-04-22T09:17:51.750898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "查询 reword 中 Linux 个版本内核贡献的详细数据，并存入 CVS 中\n",
    "'''\n",
    "\n",
    "import requests  \n",
    "import csv\n",
    "from bs4 import BeautifulSoup  \n",
    "  \n",
    "# 发送请求，获取网页内容  \n",
    "url = 'https://remword.com/kps_result/'  # 这里替换为你想要抓取的网页的 URL  \n",
    "response = requests.get(url)  \n",
    "  \n",
    "# 使用 BeautifulSoup 解析网页内容  \n",
    "soup = BeautifulSoup(response.text, 'html.parser')  \n",
    "  \n",
    "# 查找你想要的数据，这里以查找所有的段落为例  \n",
    "tr_tags = soup.find_all('table')[2].find_all('tr')\n",
    "\n",
    "kernel_data=[]\n",
    "plist=[]\n",
    "alist=[]\n",
    "\n",
    "for tr in tr_tags:\n",
    "    tds = tr.find_all('td')\n",
    "    if len(tds)>1:\n",
    "        plist=tds[0].find_all('p')\n",
    "    if len(tds)>=2:\n",
    "        alist=tds[1].find('a')\n",
    "        if alist is not None:\n",
    "            kernel_data.append({\"KernelVersion\": plist[0].text, \"URL\": \"https://remword.com/kps_result\"+alist.get('href').lstrip('.') })\n",
    "\n",
    "# 定义 CSV 文件的列名  \n",
    "fieldnames = ['KernelVersion', 'URL']\n",
    "\n",
    "# 打开文件并写入 CSV  \n",
    "with open('LinuxKernel.csv', 'w', newline='', encoding='utf-8') as csvfile:  \n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)  \n",
    "  \n",
    "    # 写入列名  \n",
    "    writer.writeheader()  \n",
    "  \n",
    "    # 写入数据行  \n",
    "    for row in kernel_data:  \n",
    "        writer.writerow(row)"
   ],
   "id": "d496d24b1e0e51b9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "使用 Linux Kernel 6.8 的中的贡献具体数据做测试，为最后的所有贡献数据做代码测试，主要是 previous_sibling 这个方法的研究\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# 发送请求，获取网页内容  \n",
    "url = 'https://remword.com/kps_result/6.8_whole.html'  # 这里替换为你想要抓取的网页的 URL  \n",
    "response = requests.get(url)  \n",
    "\n",
    "# 使用Beautiful Soup解析HTML文档\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "# 找到所有第一层级的<li>标签，并提取它们的文本值\n",
    "first_level_items = soup.find_all('ul')\n",
    "\n",
    "for li in first_level_items:\n",
    "    print(li.previous_sibling)\n"
   ],
   "id": "9328168d8efe8698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "使用 Linux Kernel 6.8 的中的贡献具体数据做测试，为最后的所有贡献数据做代码测试\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 发送请求，获取网页内容  \n",
    "url = 'https://remword.com/kps_result/6.8_whole.html'  # 这里替换为你想要抓取的网页的 URL  \n",
    "response = requests.get(url)  \n",
    "# 使用Beautiful Soup解析HTML文档\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "# 数组\n",
    "kernel_data=[]\n",
    "\n",
    "# 找到所有第一层级的<li>标签，并提取它们的文本值\n",
    "first_level_items = soup.find_all('ul')\n",
    "for li in first_level_items:\n",
    "    # 使用正则表达式匹配所需字段\n",
    "    match = re.match(r'No\\.\\d+\\s+(.+?)\\s+(\\d+)\\((\\d+\\.\\d+)%\\)', li.previous_sibling)\n",
    "    if match:\n",
    "        # 提取字段\n",
    "        label = match.group(1)\n",
    "        value = match.group(2)\n",
    "        percentage = match.group(3)\n",
    "\n",
    "        print(\"Label:\", label)\n",
    "        print(\"Value:\", value)\n",
    "        print(\"Percentage:\", percentage)\n",
    "        kernel_data.append({\"Label\": label, \"Value\": value, \"Percentage\": percentage})\n",
    "    else:\n",
    "        print(\"No match found\")\n",
    "\n",
    "# 定义 CSV 文件的列名  \n",
    "fieldnames = ['Label', 'Value','Percentage']# 打开文件并写入 CSV  \n",
    "with open('6.8_kernel_contribute_data.csv', 'w', newline='', encoding='utf-8') as csvfile:  \n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)  \n",
    "  \n",
    "    # 写入列名  \n",
    "    writer.writeheader()  \n",
    "  \n",
    "    # 写入数据行  \n",
    "    for row in kernel_data:  \n",
    "        writer.writerow(row)"
   ],
   "id": "786e9492bdd90d64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "获取 remword 中各个厂商的 Linux Kernel 贡献\n",
    "'''\n",
    "\n",
    "import requests  \n",
    "import csv\n",
    "from bs4 import BeautifulSoup  \n",
    "  \n",
    "# 发送请求，获取网页内容  \n",
    "url = 'https://remword.com/kps_result/'  # 这里替换为你想要抓取的网页的 URL  \n",
    "response = requests.get(url)  \n",
    "  \n",
    "# 使用 BeautifulSoup 解析网页内容  \n",
    "soup = BeautifulSoup(response.text, 'html.parser')  \n",
    "  \n",
    "# 查找你想要的数据，这里以查找所有的段落为例  \n",
    "tr_tags = soup.find_all('table')[2].find_all('tr')\n",
    "\n",
    "kernel_data=[]\n",
    "plist=[]\n",
    "alist=[]\n",
    "\n",
    "for tr in tr_tags:\n",
    "    tds = tr.find_all('td')\n",
    "    if len(tds)>1:\n",
    "        plist=tds[0].find_all('p')\n",
    "    if len(tds)>=2:\n",
    "        alist=tds[1].find('a')\n",
    "        if alist is not None:\n",
    "            # 发送请求，获取网页内容  \n",
    "            url = \"https://remword.com/kps_result\"+alist.get('href').lstrip('.') \n",
    "            response = requests.get(url)  \n",
    "            # 使用Beautiful Soup解析HTML文档\n",
    "            # 使用Beautiful Soup解析HTML文档\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')            \n",
    "            # 找到所有第一层级的<li>标签，并提取它们的文本值\n",
    "            first_level_items = soup.find_all('ul')\n",
    "            print('正在处理 ul 数据')\n",
    "            for li in first_level_items:\n",
    "                # 使用正则表达式匹配所需字段\n",
    "                match = re.match(r'No\\.\\d+\\s+(.+?)\\s+(\\d+)\\((\\d+\\.\\d+)%\\)', li.previous_sibling)\n",
    "                if match:\n",
    "                    # 提取字段\n",
    "                    label = match.group(1)\n",
    "                    value = match.group(2)\n",
    "                    percentage = match.group(3)\n",
    "            \n",
    "                    print(\"Label:\", label)\n",
    "                    print(\"Value:\", value)\n",
    "                    print(\"Percentage:\", percentage)\n",
    "                    kernel_data.append({'KernelVersion':plist[0].text,\"Label\": label, \"Value\": value, \"Percentage\": percentage})\n",
    "                    print(\"正在写入\"+plist[0].text+'的数据')\n",
    "                else:\n",
    "                    print(\"No match found\")\n",
    "                    \n",
    "                    \n",
    "# 定义 CSV 文件的列名  \n",
    "fieldnames = ['KernelVersion','Label', 'Value','Percentage']# 打开文件并写入 CSV  \n",
    "with open('all_kernel_contribute_data.csv', 'w', newline='', encoding='utf-8') as csvfile:  \n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)  \n",
    "  \n",
    "    # 写入列名  \n",
    "    writer.writeheader()  \n",
    "  \n",
    "    # 写入数据行  \n",
    "    for row in kernel_data:  \n",
    "        writer.writerow(row)\n",
    "    print('写入完成')"
   ],
   "id": "4640d544b574a80a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T03:06:07.441550Z",
     "start_time": "2024-04-22T03:06:06.718025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "将所有内核贡献数据导入数据库\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# 读取CSV文件\n",
    "csv_file = \"all_kernel_contribute_data.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# 连接到MariaDB数据库\n",
    "# 请确保安装了SQLAlchemy库，并提供正确的数据库连接信息\n",
    "# 格式为：dialect+driver://username:password@host:port/database\n",
    "engine = create_engine('mysql+pymysql://root:root@localhost:3306/linuxkernel')\n",
    "\n",
    "# 将数据写入数据库表\n",
    "# 假设数据表名为data_table，如果不存在会自动创建\n",
    "# 如果需要指定数据表字段类型，可以使用dtype参数\n",
    "df.to_sql('linuxkernel', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# 关闭数据库连接\n",
    "engine.dispose()\n"
   ],
   "id": "8d17803395400fa0",
   "outputs": [],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
